{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import datetime\n",
    "import subprocess\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import sql\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1 << 22).toDF(\"id\").withColumn(\"x\", rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 616 ms, total: 16.5 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%time pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 129 ms, total: 259 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%time pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/home/aaron/Downloads/20171223_v1_productlistings_00000.csv.gz,/home/aaron/Downloads/20171223_v1_productlistings_00001.csv.gz\"\n",
    "#fn = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowIOError",
     "evalue": "Failed to open local file '/home/aaron/Downloads/20171223_v1_productlistings_00000.csv.gz,/home/aaron/Downloads/20171223_v1_productlistings_00001.csv.gz', error: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowIOError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5a2bac9c8eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/_csv.pyx\u001b[0m in \u001b[0;36mpyarrow._csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/_csv.pyx\u001b[0m in \u001b[0;36mpyarrow._csv._get_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_input_stream\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._get_native_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.OSFile.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.OSFile._open_readable\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowIOError\u001b[0m: Failed to open local file '/home/aaron/Downloads/20171223_v1_productlistings_00000.csv.gz,/home/aaron/Downloads/20171223_v1_productlistings_00001.csv.gz', error: No such file or directory"
     ]
    }
   ],
   "source": [
    "table = csv.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = spark.read.csv(\"/home/aaron/Downloads/20171223_v1_productlistings_0000*.csv.gz\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/pyspark/sql/dataframe.py:2139: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o61.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n",
      "\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\n",
      "\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 21, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\n",
      "Allocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n",
      "\n",
      "\n",
      "Previous exception in task: Java heap space\n",
      "\tjava.util.Arrays.copyOf(Arrays.java:3236)\n",
      "\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n",
      "\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n",
      "\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n",
      "\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n",
      "\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n",
      "\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n",
      "\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3318)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\n",
      "\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\n",
      "\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\n",
      "Caused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\n",
      "Allocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n",
      "\n",
      "\n",
      "Previous exception in task: Java heap space\n",
      "\tjava.util.Arrays.copyOf(Arrays.java:3236)\n",
      "\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n",
      "\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n",
      "\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n",
      "\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n",
      "\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n",
      "\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n",
      "\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o61.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 21, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\nAllocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3318)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\n\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\nAllocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2120\u001b[0m                         \u001b[0m_check_dataframe_localize_timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2122\u001b[0;31m                     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collectAsArrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m                         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_collectAsArrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrowStreamSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                 \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Join serving thread and raise any exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 21, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\nAllocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3318)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\n\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (3876864)\nAllocator(toBatchIterator) 0/3876864/3905536/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:75)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:88)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:225)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:198)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply$mcV$sp(ArrowConverters.scala:118)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$anonfun$next$1.apply(ArrowConverters.scala:109)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:120)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator$class.foreach(Iterator.scala:891)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tscala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17$$anonfun$apply$18.apply(Dataset.scala:3320)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:123)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%time pdf = df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
